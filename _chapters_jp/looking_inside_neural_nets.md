---
layout: chapter
title: "ニューラルネットワークの中をのぞく"
includes: [mathjax, jquery, convnetjs, dataset, convnet, visualizer]
header_image: "/images/headers/brainbow.jpg"
header_text: "Brainbow of postnatal mouse taken <a href=\"http://www.olympusbioscapes.com/gallery/images/743\"> by Dr. Katie Matho</a>. A <a href=\"https://en.wikipedia.org/wiki/Brainbow\">brainbow</a> is a neuroimaging technique in which individual neurons are stained and visualized using fluorescent proteins."
translator: "Kynd"
translator_link: "https://twitter.com/kyndinfo"
---

[English](/ml4a/looking_inside_neural_nets/)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[español](/ml4a/es/looking_inside_neural_nets/)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[한국어](/ml4a/ko/looking_inside_neural_nets/)

[前章](/ml4a/neural_networks)では、ニューラルネットワークが訓練を通して、90%程度のかなりの精度で手書きの数字を識別できるようになる様子を見ました。この章ではネットワークの性能についてより注意深く調べるとともに、その内部の状態を詳しく観察することで、実際に何が起きているのかについてのいくつかの洞察を得ます。この章の後半では、より高いレベルに到達するにはどんな種類のイノベーションが必要になるのかを予測するために、犬や自動車や船といったより複雑な物のデータセットを使って訓練を試みることでニューラルネットワークにあえて失敗をさせてみます。
 
## 重みを可視化する

MNISTの手書き文字を分類するよう訓練したネットワークを例にとって見ましょう。前の章とは異なり、隠れ層を間に入れずに、入力層を直接出力層に繋げます。ネットワークは下の図のようになります。

{% include figure_multi.md path1="/images/figures/mnist_1layer.png" caption1="MNISTデータ判別のための1層のニューラルネットワーク、10個の出力ニューロンは0から9までの10個の数字クラスに対応する" %}

画像をニューラルネットワークに入力した際に、左下の画像のようにピクセルを一列に並べた図を描いたことを思い出してください。最初の出力ニューロンへの繋がりだけに注目して見ましょう。この出力ニューロンに$$z$$、それぞれの入力ニューロンとそれに対応する重みにそれぞれ$$x_i$$、$$w_i$$とラベルをつけます。

{% include figure_multi.md path1="/images/figures/weights_analogy_1.png" caption1="1つの出力ニューロンへの繋がりと重みだけに注目する" %}

ピクセルをまっすぐに並べる代わりに、重みをピクセルの位置にちょうど対応するように並べた28×28グリッドとして見ましょう。右上の図は下の図と異なって見えますが、同じ等式、$$z=b+\sum{w x}$$を示しています。

{% include figure_multi.md path1="/images/figures/weights_analogy_2.png" caption1="異なる方法で同じ等式を図示する：1つの出力ニューロンに対する、ピクセルと重みとの掛け合わせ" %}

この構成で訓練したネットワークを例にとって、最初の出力ニューロンに流れ込む分の重みを目に見えるようにして見ましょう。このニューロンの役割は数字の0を識別することです。重みが最も低いところが黒、最も高いところが白になるように色をつけます。

{% include figure_multi.md path1="/images/figures/rolled_weights_mnist_0.png" caption1="MNISTの0を識別するニューロンの重みを可視化する" %}

目を少し細めて見ましょう。ぼやけた0のように見えませんか。このようになる理由は、ニューロンが何をしているのかを考えるとより明確になります。このニューロンは0を識別する役割を担っているので、0に対して高い値を、それ以外に対して低い値を出力することをゴールとしています。0に対して高い値を出力するには、0の画像の中で明るくなることが多いピクセルに対して高い重みを割り当てます。また、0以外の数字に対して比較的低い値を出力するには、0以外の画像で明るいことが多く、0の画像で暗くなるピクセルに低い重みを割り当てます。重みの図の中心部分が暗くなっているのは、0の画像ではここがオフになることが多く（0の穴部分）、他の数字では大抵明るくなるからです。

10個の出力ニューロン全てについて、学習済みの重み付けを見てみましょう。予想通り、ややピンボケした10個の数字のように見えます。まるでそれぞれの数字に属する沢山の画像の平均を取ったかのようです。

{% include figure_multi.md path1="/images/figures/rolled_weights_mnist.png" caption1="MNIST分類器の全ての重みを図示する" %}

2の画像を受け取ったとったとします。2を識別する役割のニューロンは2の画像で明るくなることの多いピクセルに高い値が対応するよう重み付けされているので、出力も大きくなるだろうと予測できます。他のニューロンでも、いくつかの重みは2の画像の明るいピクセルに対応した位置にあるので、多少スコアは高くなります。しかし2を識別するニューロンと比べると重なりは少なく明るいピクセルの多くは低い重みづけによって打ち消されてしまいます。活性化関数は入力に対して単調増加、つまり入力が大きければ出力もより大きくなるので、結果としての順位に影響を与えることはありません。（訳注：最後の文がわかりにくいので補足。前章でみた通り各ニューロンでは重みとバイアスを計算した後に活性化関数で出力を調整しますが、この活性化関数は必ず単調増加という性質があります。そのため、重みとバイアスを計算した時点で2を識別するニューロンの出力が他のニューロンの出力よりも大きければ、活性化関数を通した後でもその順位は変わりません。この例のような一層のニューラルネットワークで活性化関数を使う意味はまずありません。）

重み付けは出力クラスの鋳型（テンプレート）だと考えることもできます。これはとても凄いことです。ネットワークは数字やその意味について全く事前には何も教えられていないのにも関わらず、結果としてこれらのクラスに似たものを作り上げるのですから。このことはネットワークの中で起きていることの何が本当に特別なのを示唆しています。ニューロンは学習の対象についての_表現_を作り上げるのです。この表現は単純な分類や予想よりももっと高度なことに役立てることができます。[畳み込みニューラルネットワーク](/ml4a/convnets/)について学習する際には、この表現能力をより高レベルなものに発展させます。でもまだ先回りはしないで起きましょう。

答えだけでなく沢山の疑問も湧いてきます。例えば、隠れ層を足すと重み付けには何が起きるのでしょう。すぐ後で見るように、この疑問への答えは前の説で直感的な方法でみたことを基礎として成り立ちます。これに答える前に、ニューラルネットワークの性能について、特にそれがどのような間違いを犯しやすいのかについて見ておく事が役に立ちます。

## 0op5, 1 d14 17 2ga1n（おっとまた問偉えた）

このニューラルネットワークは時折、共感できる間違い方をします。私には下の図の最初の数字が9かどうかははっきりとしません。ネットワークが間違えたのと同じく、4だと思う人も多いでしょう。同様に2つ目の数字の3が8に間違えられるのも理解できるでしょう。3つ目、4つ目の間違いはもっと明らかです。ほとんどの人は一目でこれらがそれぞれ3と2だと見分けられるでしょう。一方機械は前者を5と間違え、後者についてはほぼお手上げです。

（訳注: この感覚は国や人によって大分違うと思います。日本人の多くは4つ目の例を2だとは思わないのではないでしょうか。）

{% include figure_multi.md path1="/images/figures/mnist-mistakes.png" caption1="1層MNISTネットワークが間違えたケース。左の2つは人間にも理解できる。右の2つはより明らかな間違い" %}

MNISTの数字に対して90%の精度を出した、前章のニューラルネットワークの動作についてより詳しく見てみましょう。ネットワークの予測結果を表にした、混同行列を見るという方法があります。下に示す混同行列では横の10行がMNISTデータセットの実際のラベルに相当し、たての列がネットワークが予測したラベルに相当します。例えば4行目6列目の升目は、実際は3であるデータをニューラルネットワークが5だと間違えてラベル付けした例が71件あるということを示しています。混同行列の斜めの緑の升目は正しい予測の回数、その他の升目は間違いの回数を示しています。

マウスカーソルを升目の上に載せると、それぞれの升目からの事例がネットワークが自信を持って予測した（正解である確率が高いと予測した）順に並んで表示されます。

{% include demo_insert.html path="/demos/confusion_mnist/" parent_div="post" %}

下に示すように、混同行列のそれぞれの升目にトップの事例を並べると、より良い洞察を得る事ができます。

{% include figure_multi.md path1="/images/figures/mnist-confusion-samples.png" caption1="MNIST混同行列から、最も自信が高かった予測のサンプル" %}

これを見るとネットワークが行う予測についての大まかな概観が得られます。最初の2列を見ると、ネットワークは、ゼロを予測するには大きな輪を、1を予測するには細い線を探していて、他の数字がこのような特徴を持っている場合に間違いを犯しているようだという事が見て取れます。

## ネットワークに失敗させて見る

ここまでは、手書きの文字を識別するように訓練されたニューラルネットワークだけを見てきました。この例は多くの洞察を与えてくれますが、コンピュータにとても有利な、簡単すぎるデータセットでもあります。10個のクラスしかなく、とても明確に定義されていて、各クラス内のデータには比較的わずかの違いしかありません。ほとんどの現実のシナリオでは、もっと理想とは遠いデータを分類することになります。同じニューラルネットワークの振る舞いを異なるデータセットを使って見てみましょう。[CIFAR-10]([https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html))という、60000枚、32x32ピクセルの画像にラベルをつけたものを用います。画像は10種のクラス、飛行機、自動車、鳥、ネコ、シカ、イヌ、カエル、ウマ、船、トラックに分類されています。下図はCIFAR-10からランダムに選んだ例です。

{% include figure_multi.md path1="/images/figures/cifar-grid.png" caption1="CIFAR-10 イメージセットからのランダムな例" %}

これらの画像には、これまでに扱ったことのない違いがあると言わざるを得ないでしょう。例えばネコの画像は違う向きを向いていることもあれば、色や毛皮の模様が違うこともあり、伸びた姿勢をしていることや、丸まっていることもあれば、その他にも手書きの画像には見られなかったような様々な違いがあります。ネコの写真には他のものも散らばっていて、問題をより複雑にしています。

やはり、2層のニューラルネットワークをこれらの画像を使って訓練しても精度は37%しか出ません。それでもランダムよりはましですが（ランダムの場合は10%しか正解しない）、MNIST識別器の90%には遠く及びません。畳み込みネットワークを使う際にはMNISTとCIFAR-10の両方に対して数値を大きく改善できます。今の時点では、ネットワークの重み付けを調べることで通常のニューラルネットワークの決定についてより正確に理解します。

1層のみ、隠れ層なしのニューラルネットワークを使って先ほどと同じ重み付けを観察する実験を行って見ましょう。ただし今回はCIFAR-10の画像を使って訓練を行います。重み付けは下に示すようになります。

{% include figure_multi.md path1="/images/figures/rolled_weights_cifar.png" caption1="1層のみCIFAR-10を可視化したもの" %}

MNISTの重み付けと比較するとこれらには明確な特徴が少なく、輪郭もはるかにはっきりしていません。ディティールには直感的に納得できるものもあります。例えば飛行機と船は青空や水に囲まれていることが多いことを反映して、画像の輪郭付近が主に青色になっています。重み付けの画像はそのクラスに属する画像の平均と関連があるので、前回と同じくぼんやりとした平均の色が見られることを期待できます。しかしCIFARのクラスはクラスごとの一貫性がずっと低いので、MNISTで見られたようなはっきりしたテンプレートはなかなか現れません。

CIFAR識別器の混乱行列を見て見ましょう。

{% include demo_insert.html path="/demos/confusion_cifar/" parent_div="post" %}

やはり性能はとても低く37%にしか届きません。単純な1層のニューラルネットワークでは明らかにこのデータセットの複雑を扱うことができないのです。性能を幾らかでも改善する方法の1つは隠れ層を導入することです。次の節ではこの効果について分析します。

## 隠れ層を加える

これまでは入力が出力に直接繋がった1層のニューラルネットワークだけに注目してきました。隠れ層はニューラルネットワークにどのように影響を与えるのでしょうか。これは調べるために、MNISTネットワークに10個のニューロンから成る中間層を入れて見ましょう。このニューラルネットワークは下図のようになります。

{% include figure_multi.md path1="/images/figures/mnist_2layers.png" caption1="MNIST用の2層ニューラルネットワーク" %}

もはや784個の入力ネットワークは出力クラスに直接繋がってはいないので、1層のネットワークで使った単純なテンプレートの例えは今回はうまく当てはまりません。ある意味我々は、それぞれの重みが1つのクラスにだけ繋がりそのクラスにだけ影響するようにすることで、1層のネットワークに無理やりテンプレートを学ばせたとも言えるでしょう。しかし今回導入したようなより複雑なネットワークでは隠れ層の重み付けは出力層の10個全てのニューロンに影響します。この場合の重み付けはどのように見えるでしょう。

784個の何が起きているか理解するために、前回同様、最初のレイヤーの重み付けを可視化します。今回はまた2層目で、これらのニューロンの活性化がどのように組み合わせられて各クラスのスコアを出力するのかについても注意深く見ることにします。ある画像が1層目の特定のニューロン（フィルター）とよく適合する場合に、そのニューロンは強く活性化します。10個のニューロンは元の画像に異なる10種類の特徴がどれだけ強く現れているかを示します。出力レイヤーでは、クラスのラベルに相当する1つのニューロンは前の隠れ層での10個のニューロンの活性化を重み付けして組み合わせます。下の図を見てみましょう。

{% include demo_insert.html path="/demos/f_mnist_weights/" parent_div="post" %}

画像の一番上に示された、1層目の10個の重み付けから始めましょう。もはやこれらは画像クラスのテンプレートのようには見えず、もっと見慣れない形をしています。いくつは数字もどきのように、他のものは輪の半分、斜めの線、穴など数字の部品のように見えます。

フィルターの画像の下の行は、それぞれの画像クラスについての出力ニューロンに対応します。棒グラフは隠れ層の10個のフィルターそれぞれのに対しての重み付けを示しています。例えば0のクラスは、（0の画像が現れることの多い）外側の淵に沿って高い値を示すフィルターを好んでいるように見えます。（0の画像では穴になることが多い）中央部分に高い値をもつフィルターは嫌われています。1のクラスはほとんどこの逆で、1の縦棒が描かれることが多い中央に高い値をもつフィルターを好んでいます。

このアプローチの利点はその柔軟性にあります。それぞれのクラスに対して、対応する出力ニューロンを刺激するより幅広い入力パターンの列があります。それぞれのクラスは隠れ層でいくつかの抽象的な特徴やその組み合わせが見つかるかどうかに応じて発火します。つまり、色々な種類のゼロ、色々な種類の1、などのようにそれぞれのクラスについて学習することができるのです。このことは大抵の場合 − 必ずではないですが − ほとんどのタスクについてのネットワークの性能を改善します。

## 特徴と表現

この章で学んだことのいくつかを一般化して見ましょう。1層、または複数のニューラルネットワークでは、それぞれの層は似通った機能を果たします。レイヤーは前のレイヤーからのデータをより「高レベルの」表現に変形します。「高レベル」とは、例えば要約が本の高レベルな表現であるように、そのデータをよりコンパクトかつより際立った形で表していることを意味します。例えば上の2層のネットワークでは、1層目で「低レベル」のピクセルを数字の中に見つかる線、輪といったより「高レベル」な特徴に対応づけ、次のレイヤーでこれらの高レベルな特徴をさらに高レベルな表現、つまり実際の数字に対応づけました。このデータをより小さくもっと意味のある情報に変形するという概念こそが機械学習のもっとも大事な部分であり、ニューラルネットワークの最も重要な機能なのです。

隠れ層を加えることで、ニューラルネットワークは複数の抽象レベルで特徴を学ぶ機会を得ることができます。最初の方の層では低レベルの特徴を、後のレイヤーでは前のレイヤーの特徴を組み合わせることでより高レベルの特徴を表した、豊かなデータの表現を得ることができ流のです。

これまで見てきたように、隠れ層は精度を改善することができますが、それはある程度までに限られています。レイヤーを増やし続けても精度は早くは改善しなくなり、計算コストが上がってしまいます。ニューラルネットワークに、単純に全ての可能な種類の画像クラスを隠れ層を使って覚えておくようにお願いする、というわけには行かないのです。[畳み込みネットワーク](/ml4a/convnets)を使うというもっと良い方法があり、後の章で取り上げます。

# もっと読む（英語）

{% include further_reading.md title="Demo: Tinker with a neural network" author="Daniel Smilkov and Shan Carter" link="http://playground.tensorflow.org" %} 

{% include further_reading.md title="Visualizing what ConvNets learn (Stanford CS231n)" author="Andrej Karpathy" link="https://cs231n.github.io/understanding-cnn/" %} 

## 次の章では

次の章では、[ニューラルネットワークはどのようにして学習するのか](/ml4a/how_neural_networks_are_trained/)という、これまでごまかし続けてきた重要な話題について学びます。つまりニューラルネットワークを構築し、最急降下法やバックプロパゲーションという手法を使ってデータについて学習を行うプロセスについてです。単純な線形回帰から始めて、例を通じて学習を進め、研究者が取り扱わなくてはならない学習の様々な側面について説明することによって知識を積み上げて行きます。